---
title: CS225要点（AI生成）
date: 2025-06-04
categories: 
  - 学习笔记
tags: 
  - Data Structures
---

## Data Structures: Time Complexities of Common Methods

### Group 1: Linear Data Structures (Lists, Stacks, Queues)

| Data Structure | Implementation | Method | Average Time Complexity | Worst Time Complexity | Notes |
|---|---|---|---|---|---|
| **Array List** | Dynamic Array | `insertAtFront` | O(N) | O(N) | Requires shifting elements. Resize can be O(N). |
| | | `insertAtIndex` | O(N) | O(N) | Requires shifting elements. |
| | | `removeAtIndex` | O(N) | O(N) | Requires shifting elements. |
| | | `insertAfterElement` | O(N) | O(N) | First finds element (O(N)), then shifts (O(N)). |
| | | `removeAfterElement` | O(N) | O(N) | First finds element (O(N)), then shifts (O(N)). |
| | | `findIndex` (unsorted) | O(N) | O(N) | Linear scan. |
| | | `findIndex` (sorted) | O(log N) (Binary Search) | O(log N) (Binary Search) | Assumes data is sorted. |
| | | `findData` (unsorted) | O(N) | O(N) | Linear scan. |
| | | `findData` (sorted) | O(log N) (Binary Search) | O(log N) (Binary Search) | Assumes data is sorted. |
| | | `add` (at end) | O(1) amortized | O(N) (resize) | Amortized O(1) due to resizing. |
| | | `remove` (from end) | O(1) | O(1) | |
| **Linked List** | Nodes with Pointers | `insertAtFront` | O(1) | O(1) | Changes head pointer. |
| | | `insertAtIndex` | O(N) | O(N) | Traverses to index. |
| | | `removeAtIndex` | O(N) | O(N) | Traverses to index. |
| | | `insertAfterElement` | O(N) | O(N) | Traverses to element. |
| | | `removeAfterElement` | O(N) | O(N) | Traverses to element. |
| | | `findIndex` | O(N) | O(N) | Linear scan. |
| | | `findData` | O(N) | O(N) | Linear scan. |
| | | `add` (at end, if tail pointer) | O(1) | O(1) | Requires `tail` pointer. Otherwise O(N). |
| | | `remove` (from end, if single linked) | O(N) | O(N) | Requires traversal from head. |
| **Stack (LIFO)** | Array/List | `push` | O(1) (Array amortized) | O(N) (Array resize) | Array-based stack can resize. |
| | | `pop` | O(1) | O(1) | |
| | | `top/peek` | O(1) | O(1) | |
| **Queue (FIFO)** | Array/List | `enqueue` | O(1) (Array amortized) | O(N) (Array resize) | Array-based queue can resize. |
| | | `dequeue` | O(1) (Circular Array/Linked List) | O(1) (Circular Array/Linked List) | Array: Can be O(N) if elements shifted. Circular array or linked list provides O(1). |
| | | `front/peek` | O(1) | O(1) | |

### Group 2: Hierarchical Data Structures (Trees)

| Data Structure | Method | Average Time Complexity | Worst Time Complexity | Notes |
|---|---|---|---|---|
| **Binary Search Tree (BST)** | `find` | O(h) | O(N) | `h` is height. Worst case is skewed tree (like linked list). |
| | `insert` | O(h) | O(N) | `h` is height. Worst case is skewed tree. |
| | `delete` | O(h) | O(N) | `h` is height. Worst case is skewed tree. |
| | `traverse` (Pre/In/Post/Level) | O(N) | O(N) | Visits each node once. |
| **AVL Tree (Self-Balancing BST)** | `find` | O(log N) | O(log N) | `h` is always O(log N) due to balancing. |
| | `insert` | O(log N) | O(log N) | `h` is always O(log N) due to balancing. Involves rotations. |
| | `delete` | O(log N) | O(log N) | `h` is always O(log N) due to balancing. Involves rotations. |
| **Heap (Min-Heap/Max-Heap)** | `buildHeap` (from array) | O(N) | O(N) | Done Bottom-Up. |
| | `insert` | O(log N) | O(log N) | `heapifyUp` operation. |
| | `removeMin/Max` | O(log N) | O(log N) | `heapifyDown` operation. |
| | `peekMin/Max` | O(1) | O(1) | |
| **B-Tree** | `find` | O(log_m N) | O(log_m N) | `m` is order of B-tree (branching factor). `h` is O(log_m N). |
| | `insert` | O(log_m N) | O(log_m N) | Can involve splits and promotions. |
| | `delete` | O(log_m N) | O(log_m N) | Can involve merges and borrowing. |
| **k-d Tree** | `nearestNeighborSearch` | O(log N) (average) | O(N) (worst) | Depends on data distribution and search region. |
| | `rangeSearch` | O(sqrt(N)) (average) | O(N) (worst) | Depends on data distribution and search region. |

### Group 3: Hash Tables (Dictionaries)

| Data Structure | Collision Strategy | Method | Average Time Complexity | Worst Time Complexity | Notes |
|---|---|---|---|---|---|
| **Hash Table** | (General) | `insert` | O(1) | O(N) | Worst case: all keys hash to same bucket. |
| | | `find` | O(1) | O(N) | Worst case: all keys hash to same bucket. |
| | | `delete` | O(1) | O(N) | Worst case: all keys hash to same bucket. |
| | Separate Chaining | `insert` | O(1) | O(N) | `N` is number of elements in longest chain. |
| | | `find` | O(1) | O(N) | `N` is number of elements in longest chain. |
| | | `delete` | O(1) | O(N) | `N` is number of elements in longest chain. |
| | Linear Probing | `insert` | O(1) | O(N) | Suffers from primary clustering. |
| | | `find` | O(1) | O(N) | Suffers from primary clustering. |
| | | `delete` | O(1) | O(N) | Deletion often involves "tombstones". |
| | Double Hashing | `insert` | O(1) | O(N) | Reduces clustering compared to linear probing. |
| | | `find` | O(1) | O(N) | Reduces clustering. |
| | | `delete` | O(1) | O(N) | |
| | `re-hashing` | O(N) | O(N) | Done when load factor exceeds threshold. Creates new, larger table. |

### Group 4: Disjoint Sets

| Data Structure | Method | Average Time Complexity | Worst Time Complexity | Notes |
|---|---|---|---|---|
| **Disjoint Set (Union-Find)** | `find` (with path compression) | O(alpha(N)) | O(log N) | Alpha(N) is inverse Ackermann function, practically constant. |
| | `union` (by size/height + path compression) | O(alpha(N)) | O(log N) | Alpha(N) is inverse Ackermann function, practically constant. |

### Group 5: Graphs

| Data Structure | Implementation | Method | Time Complexity | Notes |
|---|---|---|---|---|
| **Graph Traversals** | Adjacency List | `BFS` | O(V + E) | `V` vertices, `E` edges. Uses a queue. |
| | Adjacency Matrix | `BFS` | O(V^2) | Checks all `V` possible edges for each vertex. |
| | Adjacency List | `DFS` | O(V + E) | Uses a stack or recursion. |
| | Adjacency Matrix | `DFS` | O(V^2) | Checks all `V` possible edges for each vertex. |

## Algorithms: Pseudocode and Complexity

### Recursion

**Concept:** A function that calls itself. Must have a base case to terminate and a recursive step that moves towards the base case.

**General Pseudocode Structure:**

```
function recursive_function(parameters):
    if base_case_condition(parameters):
        return base_case_result
    else:
        # Perform some work
        # Call recursive_function with modified parameters
        return result_of_recursive_call + current_work
```

**Complexity:** Varies greatly depending on the problem. Often analyzed using recurrence relations.
*   **Linear Recursion:** O(N) (e.g., factorial)
*   **Tree Recursion:** O(2^N) or O(branches^depth) (e.g., Fibonacci without memoization)
*   **Tail Recursion:** Can sometimes be optimized by compilers to O(N) space.

### Minimum Spanning Tree (MST) Algorithms

**Goal:** Find a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight.

#### Kruskal's Algorithm

**Concept:** Greedily adds the cheapest available edge that does not form a cycle. Uses a Disjoint Set data structure to detect cycles.

**Pseudocode:**

1.  Create a list of all edges in the graph.
2.  Sort all edges in non-decreasing order of their weights.
3.  Initialize a Disjoint Set data structure where each vertex is in its own set.
4.  Initialize an empty MST set.
5.  For each edge (u, v) from the sorted list:
    a.  If `find(u)` is not equal to `find(v)` (i.e., u and v are in different sets):
        i.   Add edge (u, v) to the MST set.
        ii.  Perform `union(u, v)`.
6.  Return the MST set.

**Time Complexity:**
*   Sorting edges: O(E log E)
*   Disjoint Set operations: O(E * alpha(V)) (for E edges, V vertices with path compression and union by size/rank)
*   **Total:** O(E log E) or O(E log V) (since E <= V^2, log E is often similar to log V)

#### Prim's Algorithm

**Concept:** Starts from an arbitrary vertex and grows the MST by adding the cheapest edge from a vertex in the MST to a vertex outside the MST. Typically uses a min-priority queue.

**Pseudocode:**

1.  Initialize a min-priority queue `PQ` to store edges.
2.  Initialize `dist[v] = infinity` for all vertices `v`, and `parent[v] = null`.
3.  Choose a starting vertex `s`. Set `dist[s] = 0`.
4.  Add `(0, s)` (weight, vertex) to `PQ`.
5.  Initialize `visited[v] = false` for all vertices.
6.  While `PQ` is not empty:
    a.  Extract the edge `(weight, u)` with the minimum weight from `PQ`.
    b.  If `visited[u]` is true, continue (already processed).
    c.  Set `visited[u] = true`.
    d.  For each neighbor `v` of `u` (with edge weight `w(u,v)`):
        i.   If `visited[v]` is false and `w(u,v) < dist[v]`:
            1.  Set `dist[v] = w(u,v)`.
            2.  Set `parent[v] = u`.
            3.  Add `(dist[v], v)` to `PQ`.
7.  The MST is formed by the edges (parent[v], v) for all v != s.

**Time Complexity:**
*   Using Binary Heap (Adj List): O(E log V)
*   Using Fibonacci Heap (Adj List): O(E + V log V)

### Shortest Path Algorithms

**Goal:** Find the path between two vertices (or from one vertex to all others) such that the sum of the edge weights along the path is minimized.

#### Dijkstra's Algorithm

**Concept:** Finds the shortest paths from a single source vertex to all other vertices in a graph with non-negative edge weights. Uses a min-priority queue.

**Pseudocode:**

1.  Initialize `dist[v] = infinity` for all vertices `v`, and `dist[source] = 0`.
2.  Initialize a min-priority queue `PQ` and add `(0, source)` (distance, vertex) to it.
3.  Initialize `visited[v] = false` for all vertices.
4.  While `PQ` is not empty:
    a.  Extract the vertex `u` with the minimum `dist` value from `PQ`.
    b.  If `visited[u]` is true, continue.
    c.  Set `visited[u] = true`.
    d.  For each neighbor `v` of `u` (with edge weight `w(u,v)`):
        i.   If `dist[u] + w(u,v) < dist[v]`:
            1.  Set `dist[v] = dist[u] + w(u,v)`.
            2.  Add `(dist[v], v)` to `PQ`.
5.  `dist[v]` now holds the shortest distance from `source` to `v`.

**Time Complexity:**
*   Using Binary Heap (Adjacency List): O(E log V)
*   Using Fibonacci Heap (Adjacency List): O(E + V log V)
*   Using Array (Adjacency Matrix): O(V^2)

#### Floyd-Warshall Algorithm

**Concept:** Finds shortest paths between all pairs of vertices in a weighted graph. Can handle negative edge weights, but no negative cycles. It's a dynamic programming approach.

**Pseudocode:**

1.  Initialize a `dist[V][V]` matrix where `dist[i][j]` is:
    *   `w(i,j)` if an edge exists between `i` and `j`.
    *   `0` if `i == j`.
    *   `infinity` if no direct edge exists.
2.  For `k` from 0 to `V-1` (intermediate vertices):
    a.  For `i` from 0 to `V-1` (source vertices):
        b.  For `j` from 0 to `V-1` (destination vertices):
            i.   `dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])`

**Time Complexity:** O(V^3)

This summary provides a comprehensive overview of the data structures and algorithms covered, along with their key properties, complexities, and pseudocode where relevant.